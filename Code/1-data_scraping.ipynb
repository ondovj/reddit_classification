{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Subreddits by Title Content\n",
    "\n",
    "### Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit can sometimes be a confusing place. With so many communities specializing in so many different topics, it can easily be overwhelming to keep different subreddits straight. To this end, it could be useful to have a model that could take a specific post, and predict which subreddit it came from. Specifically, we want a model to be able to classify posts between two gaming subreddits, r/gaming and r/pcgaming. Gaming is a general gaming subreddit, mostly centered around video games, whereas PCgaming is cetnered specifically around video games played on PCs, and the PCs that are built to play those games. \n",
    "\n",
    "In order to build this model, posts from each subreddit will be required, and the language in those posts will need to be analyzed. This is specifically a binary classification problem, so the modeling techniques used must be able to make this kind of prediction. In order to compare the different models, the accuracy metric will be utilized. Ideally, this model will have a significant boost in performance compared to a baseline mode prediction, and adapt to new data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this project was to acquire the necessary data, and attempt to pull 1000 posts from each targeted subreddit. In the end, we found that 40-50% of the posts that we pulled from each subreddit had been repeated.\n",
    "\n",
    "The real work began when the target information (the post title) had been isolated. Each of the posts had unique words and language patterns that could help to decipher which subreddit it had been from. In order to fully analyze this information we had to use NLP (natural language processing). This was an iterative and multi-step process that included stemming words down to their base roots, removing so-called \"stop words\" in order to access the words conveying more meaning, and even removing some of the words that were very frequent in both of the subreddits (such as \"game\", \"gaming, and \"new\"). \n",
    "\n",
    "As there are many modeling techniques available to solve a classification problem, it was important to be able to optimize each model, and then compare the best models against each other. To this end, we used pipelines and gridsearches to perform large groups of fits at a time. As always, a `train_test_split` was used to keep a holdout dataset from the model for testing purposes. Two vectorizers were utilized, `CountVectorizer` and `TfidfVectorizer` in order to optimize the way that each title's words were incorporated. For the classifiers, six different types were utilized: `LogisticRegression`, `KNearestNeighbors`, `MultinomialNB`, `RandomForestClassifier`, `AdaboostClassifier`, and `SVC` (Support Vector Classifier). Each of these models have certain benefits and pitfalls, and the results of each were somewhat unexpected. \n",
    "\n",
    "In the end, The model that was able to optimize accuracy, and still have a reasonable balance between bias and variance, utilized `TfidfVectorizer` and `SVC`. This model was able to give a 78.3% accuracy on the testing set. We believe this model is effective enough to continue classifying other posts between these two subreddits, mainly due to the fact that this case has fairly low stakes for misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importing Packages](#Importing-Packages)\n",
    "2. [Scraping Data](#Scraping-Data)\n",
    "    1. [Scraping r/gaming](#Scraping-r/gaming)\n",
    "    2. [Scraping r/pcgaming](#Scraping-r/pcgaming)\n",
    "3. [Saving Data](#Saving-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things that might be useful\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit's API will allow people to pull up to 1000 posts from each subreddit. However, each typical request for posts only returns 25 entries. Since we need a large amount of data to be able to create a well functioning model we can either pull 25 entries a lot of times, or try to pull more entries a few times.\n",
    "\n",
    "With either method, we want to create a custom function to do the pulls for us, without pinging reddit so much that we get blocked. Pulling a larger amount of posts each time also reduces how much we have to ping reddit, hopefully also reducing the chances of getting blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping r/gaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the url/request info for the first subreddit\n",
    "# changing post limit to 100 and using params dict from Madhi's wisdom\n",
    "gaming_url = 'https://www.reddit.com/r/gaming.json' # API Endpoint\n",
    "\n",
    "# setting params dict to increase posts per pull, and update the link to grab new posts\n",
    "params = {\"limit\": 100}\n",
    "user_agent = {'User-agent': 'jondov'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list for all the posts from r/gaming\n",
    "gaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom function to take in vars and make the right pulls\n",
    "def reddit_puller(url, params, user_agent, pulls, post_list):\n",
    "    # adapting code from Boom Devahastin Na Ayudhya\n",
    "    \n",
    "    for pull_num in range(int(pulls)):\n",
    "        \n",
    "        # stating which pull is being attempted\n",
    "        print(\"Pulling data attempted\", pull_num+1, \"times\")\n",
    "        \n",
    "        # establishing the request code\n",
    "        res = requests.get(url, headers=user_agent, params=params)\n",
    "        \n",
    "        # pull the correct data if the code is good\n",
    "        if res.status_code == 200:\n",
    "            json_data = res.json()                      #  Pull JSON\n",
    "            post_list.extend(json_data['data']['children']) #  Get posts and extend the `posts` list \n",
    "            \n",
    "            # updating url with the id of the last post in the pull\n",
    "            # next pull will grab the following entries\n",
    "            after = json_data['data']['after']\n",
    "            params[\"after\"] = after\n",
    "        \n",
    "        else:\n",
    "            print(\"There has been an error. The code is: \", res.status_code)\n",
    "            break\n",
    "            \n",
    "        # sleeping the func so we aren't locked out\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# running the new function\n",
    "reddit_puller(gaming_url, params, user_agent, 10, gaming_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pulling appears to have been completed successfully, but now we have to verify what we ended up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making func to check total and unique posts scraped\n",
    "def post_checker(post_list):\n",
    "    print(f\"Total posts scraped: {len(post_list)}.\")\n",
    "    print(\"We have:\", len(set([p['data']['id'] for p in post_list])), \"unique posts for this subreddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many posts were scraped\n",
    "len(gaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have: 645 unique posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking how many of the scraped posts have unique ids\n",
    "print(\"We have:\", len(set([p['data']['id'] for p in gaming_posts])), \"unique posts in this subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we did not actually end up with as many posts as thought. There is most likely something in the pulling request or the reddit API that is not properly pulling the posts in the sequential fashion that we wanted.\n",
    "\n",
    "This issue is beyond the scope of the project, however, as we can still work with the amount of unique posts that we have here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Scraping for r/gaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few months since the project was completed, new data is going to be pulled to add onto the original dataset and improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making new list for posts\n",
    "new_gaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# making new pull 9/23/2019\n",
    "reddit_puller(gaming_url, params, user_agent, 10, new_gaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts scraped: 967.\n",
      "We have: 566 unique posts for this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking the new post list\n",
    "post_checker(new_gaming_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping r/pcgaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process as above is now going to be repeated for r/pcgaming. We only need to make a new base url and list to store the posts, as the `params` and `user_agent` will stay the same, and the function will take care of the rest for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the url for the second subreddit\n",
    "pcgaming_url = 'https://www.reddit.com/r/pcgaming.json' # API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the list to store all the posts from r/pcgaming\n",
    "pcgaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# running func on new subreddit\n",
    "reddit_puller(pcgaming_url, params, user_agent, 10, pcgaming_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second pull was also successful, and we should now also check how many unique posts we have against the total pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have: 583 unique posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking how many of the scraped posts have unique ids\n",
    "print(\"We have:\", len(set([p['data']['id'] for p in pcgaming_posts])), \"unique posts in this subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same issue with this subreddit as well. It is unfortunate that we were not able to pull the full amount of unique posts for each subreddit, but the model should still be effective with over 1200 total posts for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Scraping for r/pcgaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional scraping will also be performed for the pcgaming subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making new list for posts\n",
    "new_pcgaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# making new pull 9/23/2019\n",
    "reddit_puller(pcgaming_url, params, user_agent, 10, new_pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts scraped: 1296.\n",
      "We have: 648 unique posts for this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking the new post list\n",
    "post_checker(new_pcgaming_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the posts need to be converted into something we can work with visually. Dataframes will do nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframes of what was scraped\n",
    "gaming_posts_df = pd.DataFrame(gaming_posts)\n",
    "pcgaming_posts_df = pd.DataFrame(pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(945, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data kind\n",
       "0  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "1  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "2  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "3  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "4  {'approved_at_utc': None, 'subreddit': 'gaming...   t3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gaming_posts_df.shape)\n",
    "gaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(985, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data kind\n",
       "0  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "1  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "2  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "3  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "4  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pcgaming_posts_df.shape)\n",
    "pcgaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two dataframes of all the information from each pulled post. They're not quite in the final form that we need them, but it is a good start.\n",
    "\n",
    "We will repeat the process for the new posts that were pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframes of new scraped posts 9/23/2019\n",
    "new_gaming_posts_df = pd.DataFrame(new_gaming_posts)\n",
    "new_pcgaming_posts_df = pd.DataFrame(new_pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kind                                               data\n",
       "0   t3  {'approved_at_utc': None, 'subreddit': 'gaming...\n",
       "1   t3  {'approved_at_utc': None, 'subreddit': 'gaming...\n",
       "2   t3  {'approved_at_utc': None, 'subreddit': 'gaming...\n",
       "3   t3  {'approved_at_utc': None, 'subreddit': 'gaming...\n",
       "4   t3  {'approved_at_utc': None, 'subreddit': 'gaming..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_gaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kind                                               data\n",
       "0   t3  {'approved_at_utc': None, 'subreddit': 'pcgami...\n",
       "1   t3  {'approved_at_utc': None, 'subreddit': 'pcgami...\n",
       "2   t3  {'approved_at_utc': None, 'subreddit': 'pcgami...\n",
       "3   t3  {'approved_at_utc': None, 'subreddit': 'pcgami...\n",
       "4   t3  {'approved_at_utc': None, 'subreddit': 'pcgami..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pcgaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good place to save the progress. Saving these to files will allow us to continue working with these datasets without having to pull the posts from reddit again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the new data as it was scraped so it doesn't get lost\n",
    "new_gaming_posts_df.to_csv(\"../datasets/gaming_posts.csv\", mode=\"a\", index=False, index_label=False)\n",
    "new_pcgaming_posts_df.to_csv(\"../datasets/pcgaming_posts.csv\", mode=\"a\", index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the dataframes hold the full information of each post, though what we want is only the title of the unique posts that were scraped. We can loop through the dictionary of each post to extract the titles, and then only keep one of each unique title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making list of just the titles from each post\n",
    "# getting set of that list to remove duplicates\n",
    "unique_titles = set([new_gaming_posts_df[\"data\"][post][\"title\"] for post in range(len(new_gaming_posts_df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list can now be turned into a dataframe to more easily view/work on it. This is a good time to add our \"target\" column, a binary classification of which subreddit the post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pc_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who's your favorite and most trustworthy game ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mario Maker 2 in a nut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finish adding backlight LCD screen to my gameb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Just Five Days Borderlands 3 Became 2K's Fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whats a game everyone hates but you like?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  pc_sub\n",
       "0  Who's your favorite and most trustworthy game ...       0\n",
       "1                             Mario Maker 2 in a nut       0\n",
       "2  Finish adding backlight LCD screen to my gameb...       0\n",
       "3  In Just Five Days Borderlands 3 Became 2K's Fa...       0\n",
       "4          Whats a game everyone hates but you like?       0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a new df of the titles, and creating a subreddit dummy column\n",
    "new_gaming_titles_df = pd.DataFrame(unique_titles, columns=[\"title\"])\n",
    "new_gaming_titles_df[\"pc_sub\"] = 0\n",
    "new_gaming_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pc_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just built PC but can’t seem to finish all the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So the rockstar games launcher</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shenmue 3 Kickstarter backers can still reques...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early Access release date for Chernobylite ann...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands 3 - DX11 and DX12 testing - 3900X ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  pc_sub\n",
       "0  Just built PC but can’t seem to finish all the...       1\n",
       "1                     So the rockstar games launcher       1\n",
       "2  Shenmue 3 Kickstarter backers can still reques...       1\n",
       "3  Early Access release date for Chernobylite ann...       1\n",
       "4  Borderlands 3 - DX11 and DX12 testing - 3900X ...       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeating the process for the pcgaming df\n",
    "unique_titles = set([new_pcgaming_posts_df[\"data\"][post][\"title\"] for post in range(len(new_pcgaming_posts_df))])\n",
    "new_pcgaming_titles_df = pd.DataFrame(unique_titles, columns=[\"title\"])\n",
    "new_pcgaming_titles_df[\"pc_sub\"] = 1\n",
    "new_pcgaming_titles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two dataframes of just post titles, matched with their respective subreddit. With these dataframes, we can begin the process of exploring and cleaning the data. This seems like another good spot to save the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending these new dfs to the old csv files\n",
    "new_gaming_titles_df.to_csv(\n",
    "    \"../datasets/gaming_titles.csv\",\n",
    "    mode=\"a\",\n",
    "    index=False,\n",
    "    label_index=False\n",
    ")\n",
    "new_pcgaming_titles_df.to_csv(\n",
    "    \"../datasets/pcgaming_titles.csv\",\n",
    "    mode=\"a\",\n",
    "    index=False,\n",
    "    label_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preserve these cells, and not constantly re-pull the posts, the remainder of the project will be in a new notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
