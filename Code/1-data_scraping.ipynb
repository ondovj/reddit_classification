{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Subreddits by Post Content\n",
    "\n",
    "### Notebook 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit can sometimes be a confusing place. With so many communities specializing in so many different topics, it can easily be overwhelming to keep different subreddits straight. To this end, it could be useful to have a model that could take a specific post, and predict which subreddit it came from. Specifically, we want a model to be able to classify posts between two gaming subreddits, r/gaming and r/pcgaming. Gaming is a general gaming subreddit, mostly centered around video games, whereas PCgaming is cetnered specifically around video games played on PCs, and the PCs that are built to play those games. \n",
    "\n",
    "In order to build this model, posts from each subreddit will be required, and the lanuage in those posts will need to be analyzed. This is specifcally a binary classification problem, so the modeling techniques used must be able to make this kind of prediction. In order to compare the differnet models, the accuracy metric will be utilized. Ideally, this model will have a significant boost in performance compared to a baseline mode prediction, and adapt to new data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this project was to acquire the necessary data, and attempt to pull 1000 posts from each targeted subreddit. In the end, we found that 40-50% of the posts that we pulled from each subreddit had been repeated.\n",
    "\n",
    "The real work began when the target information (the post title) had been isolated. Each of the posts had unique words and language patterns that could help to decipher which subreddit it had been from. In order to fully analyze this information we had to use NLP (natural language processing). This was an iterative and multi-step process that included stemming words down to their base roots, removing so-called \"stop words\" in order to access the words conveying more meaning, and even removing some of the words that were very frequent in both of the subreddits (such as \"game\", \"gaming, and \"new\"). \n",
    "\n",
    "As there are many modeling techniques available to solve a classification problem, it was important to be able to optimize each model, and then compare the best models against each other. To this end, we used pipelines and gridsearches to perform hundreds of fits at a time. As always, a `train_test_split` was used to keep a holdout dataset from the model for testing purposes. Two vectorizers were utilized, `CountVectorizer` and `TfidfVectorizer` in order to optimize the way that each title's words were incorported. For the classifiers, five different types were utilized: `LogisticRegression`, `KNearestNeighbors`, `MultinomialNB`, `RandomForestClassifier`, and `AdaboostClassifier`. Each of these models have certian benefits and pitfalls, and the results of each were somehwat unexpected. \n",
    "\n",
    "In the end, The model that was able to optimize accuracy, and still have a reasonable balance between bias and variance, utilized `TfidfVectorizer` and `AdaboostClassifier`. This model was able to give a 78.4% accuracy on the testing set. We believe this model is effective enough to continue classifying other psots between these two subreddits, mainly due tot eh fact that this case has fairly low stakes for misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importing Packages](#Importing-Packages)\n",
    "2. [Scraping Data](#Scraping-Data)\n",
    "    1. [Scraping r/gaming](#Scraping-r/gaming)\n",
    "    2. [Scraping r/pcgaming](#Scraping-r/pcgaming)\n",
    "3. [Saving Data](#Saving-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the things that might be useful\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit's API will allow people to pull up to 1000 posts from each subreddit. However, each typical request for posts only returns 25 entries. Since we need a large amount of data to be able to create a well functioning model we can either pull 25 entries a lot of times, or try to pull more entries a few times.\n",
    "\n",
    "With either method, we want to create a function to do the pulls for us, without pinging reddit so much that we get blocked. Pulling a larger amount of posts each time also reduces how much we have to ping reddit, hopefully also reducing the chances of getting blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping r/gaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the url/request info for the first subreddit\n",
    "# changing post limit to 100 and using params dict from Madhi's wisdom\n",
    "gaming_url = 'https://www.reddit.com/r/gaming.json' # API Endpoint\n",
    "\n",
    "# setting params dict to increase posts per pull, and update the link to grab new posts\n",
    "params = {\"limit\": 100}\n",
    "user_agent = {'User-agent': 'jondov'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list for all the posts from r/gaming\n",
    "gaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# copypasta from Boom's local session\n",
    "# minor changes made\n",
    "\n",
    "# making the pull for r/gaming\n",
    "for pull_num in range(10):\n",
    "\n",
    "    ##### PREPARATIONS #####\n",
    "    \n",
    "    # Create some kind of message to tell us which request number we're at\n",
    "    print(\"Pulling data attempted\", pull_num+1, \"times\")\n",
    "    \n",
    "    \n",
    "    ##### PULLING REQUEST AND EXTRACTING THE DATA #####\n",
    "    \n",
    "    # Step 1: Make request\n",
    "    res = requests.get(gaming_url, headers=user_agent, params=params)\n",
    "    \n",
    "\n",
    "    # Step 2: Extract just the data that we need\n",
    "    if res.status_code == 200:\n",
    "        json_data = res.json()                     #  Pull JSON\n",
    "        gaming_posts.extend(json_data['data']['children']) #  Get posts and extend the `posts` list\n",
    "        \n",
    "    # Step 3: Update the after param for next loop - grabs the next set of posts\n",
    "        after = json_data['data']['after']\n",
    "        params[\"after\"] = after\n",
    "        # 'after' = ID of the last post in this pull iteration\n",
    "    else:\n",
    "        print(\"We've run into an error. The status code is:\", res.status_code)\n",
    "        break\n",
    "\n",
    "    # Create a brief pause so the API doesn't lock you out by mistaking you for a machine\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pulling appears to have been completed successfully, but now we have to verify what we ended up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how many posts were scraped\n",
    "len(gaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have: 645 unique posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking how many of the scraped posts have unique ids\n",
    "print(\"We have:\", len(set([p['data']['id'] for p in gaming_posts])), \"unique posts in this subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we did not actually end up with as many posts as thought. There is most likely something in the pulling request or the reddit api that is not properly pulling the posts in the sequential fashion that we wanted.\n",
    "\n",
    "This issue is beyond the scope of the project, however, and we can still work with the amount of unique posts that we have here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping r/pcgaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process as above is now going to be repeated for r/pcgaming. The only changes to the code here are to make that distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the url/request info for the second subreddit\n",
    "pcgaming_url = 'https://www.reddit.com/r/pcgaming.json' # API Endpoint\n",
    "params = {\"limit\": 100}\n",
    "user_agent = {'User-agent': 'jondov'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the list to store all the posts from r/pcgaming\n",
    "pcgaming_posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data attempted 1 times\n",
      "Pulling data attempted 2 times\n",
      "Pulling data attempted 3 times\n",
      "Pulling data attempted 4 times\n",
      "Pulling data attempted 5 times\n",
      "Pulling data attempted 6 times\n",
      "Pulling data attempted 7 times\n",
      "Pulling data attempted 8 times\n",
      "Pulling data attempted 9 times\n",
      "Pulling data attempted 10 times\n"
     ]
    }
   ],
   "source": [
    "# same loop as above\n",
    "\n",
    "# making the pull for r/pcgaming\n",
    "for pull_num in range(10):\n",
    "\n",
    "    ##### PREPARATIONS #####\n",
    "    \n",
    "    # Create some kind of message to tell us which request number we're at\n",
    "    print(\"Pulling data attempted\", pull_num+1, \"times\")\n",
    "    \n",
    "    \n",
    "    ##### PULLING REQUEST AND EXTRACTING THE DATA #####\n",
    "    \n",
    "    # Step 1: Make request\n",
    "    res = requests.get(pcgaming_url, headers=user_agent, params=params)\n",
    "    \n",
    "\n",
    "    # Step 2: Extract data (but may want to extract some)\n",
    "    if res.status_code == 200:\n",
    "        json_data = res.json()                     #  Pull JSON\n",
    "        pcgaming_posts.extend(json_data['data']['children']) #  Get posts and extend the `posts` list\n",
    "        \n",
    "    # Step 3: Update the after string for next loop\n",
    "        after = json_data['data']['after']\n",
    "        params[\"after\"] = after\n",
    "        # 'after' = ID of the last post in this pull iteration\n",
    "    else:\n",
    "        print(\"We've run into an error. The status code is:\", res.status_code)\n",
    "        break\n",
    "\n",
    "    # Create a brief pause so the API doesn't lock you out by mistaking you for a machine\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have: 583 unique posts in this subreddit\n"
     ]
    }
   ],
   "source": [
    "# checking how many of the scraped posts have unique ids\n",
    "print(\"We have:\", len(set([p['data']['id'] for p in pcgaming_posts])), \"unique posts in this subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same issue with this subreddit as well. It is unfortunate that we were not able to pull the full amount of unique posts for each subreddit, but the model should still be effective with over 1200 total posts for our dataset.\n",
    "\n",
    "Now the posts need to be converted into something we can work with visually. Dataframes will do nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataframes of what was scraped\n",
    "gaming_posts_df = pd.DataFrame(gaming_posts)\n",
    "pcgaming_posts_df = pd.DataFrame(pcgaming_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(945, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'gaming...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data kind\n",
       "0  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "1  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "2  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "3  {'approved_at_utc': None, 'subreddit': 'gaming...   t3\n",
       "4  {'approved_at_utc': None, 'subreddit': 'gaming...   t3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gaming_posts_df.shape)\n",
    "gaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(985, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'pcgami...</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data kind\n",
       "0  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "1  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "2  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "3  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3\n",
       "4  {'approved_at_utc': None, 'subreddit': 'pcgami...   t3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pcgaming_posts_df.shape)\n",
    "pcgaming_posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two dataframes of all the information from each pulled post. THey're not quite in the final form that we need them, but it is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good place to save the progress. Saving these to files will allow us to continue working with these datasets without having to pull the posts from reddit again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data as it was scraped so it doesn't get lost\n",
    "gaming_posts_df.to_csv(\"../datasets/gaming_posts.csv\", index=False)\n",
    "pcgaming_posts_df.to_csv(\"../datasets/pcgaming_posts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the dataframes hold the full information of each post, though what we want is only the title of the unique posts that were scraped. We can loop through the dictionary of each post to extract the titles, and then only keep one of each unique title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making list of just the titles from each post\n",
    "# getting set of that list to remove duplicates\n",
    "unique_titles = set([gaming_posts_df[\"data\"][post][\"title\"] for post in range(len(gaming_posts_df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list can now be turned into a dataframe to more easily view/work on it. This is a good time to add our \"target\" column, a binary classification of which subreddit the post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pc_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two Lord of the Rings Inspired Mario Maker Lev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Request for \"offline mmo\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This sub is extremely hypocritical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chapter 15 of Evil Within is kicking my butt//...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Turning Inferno into Dust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  pc_sub\n",
       "0  Two Lord of the Rings Inspired Mario Maker Lev...       0\n",
       "1                          Request for \"offline mmo\"       0\n",
       "2                 This sub is extremely hypocritical       0\n",
       "3  Chapter 15 of Evil Within is kicking my butt//...       0\n",
       "4                          Turning Inferno into Dust       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a new df of the titles, and creating a subreddit dummy column\n",
    "gaming_titles_df = pd.DataFrame(unique_titles, columns=[\"title\"])\n",
    "gaming_titles_df[\"pc_sub\"] = 0\n",
    "gaming_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pc_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crackdown 3 Flying High Update (Official Trailer)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New games coming to Gamepass. Shadow of War, D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Buckyball Tournament is Coming to Steam ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skyrim’s murky seas finally get an overhaul un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nvidia RTX 2060 Super/ RTX 2070 Super Review! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  pc_sub\n",
       "0  Crackdown 3 Flying High Update (Official Trailer)       1\n",
       "1  New games coming to Gamepass. Shadow of War, D...       1\n",
       "2  Super Buckyball Tournament is Coming to Steam ...       1\n",
       "3  Skyrim’s murky seas finally get an overhaul un...       1\n",
       "4  Nvidia RTX 2060 Super/ RTX 2070 Super Review! ...       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeating the process for the pcgaming df\n",
    "unique_titles = set([pcgaming_posts_df[\"data\"][post][\"title\"] for post in range(len(pcgaming_posts_df))])\n",
    "pcgaming_titles_df = pd.DataFrame(unique_titles, columns=[\"title\"])\n",
    "pcgaming_titles_df[\"pc_sub\"] = 1\n",
    "pcgaming_titles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two dataframes of just post titles, matched with their respective subreddit. With these dataframes, we can begin the process of exloring and cleaning the data. This seems like another good spot to save the progess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving these new dfs to csv files\n",
    "gaming_titles_df.to_csv(\"../datasets/gaming_titles.csv\", index=False)\n",
    "pcgaming_titles_df.to_csv(\"../datasets/pcgaming_titles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preserve these cells, and not constantly re-pull the posts, the remainder of the project will be in a new notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
